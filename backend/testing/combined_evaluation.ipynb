{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Evaluation Notebook\n",
    "\n",
    "This notebook computes:\n",
    "\n",
    "1. **Main evaluation (ISO/IEC 27002 & 27017)**\n",
    "   - Metrics: Consistency, Completeness, Correctness, Generalizability, Precision\n",
    "   - Method mapping:\n",
    "     - 27002 ‚Üí Method1 = SSSL, Method2 = LLM\n",
    "     - 27017 ‚Üí Method1 = KNN (derived), Method2 = LLM\n",
    "\n",
    "2. **Other evaluation (Query comparison)**\n",
    "   - Method1 = LLM, Method2 = Similarity\n",
    "   - Computes averages and winner per Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FILE = r'C:\\Users\\anoureldin\\Downloads\\Measuring Semantic Label Accuracy and Retrieval Precision (Responses) (1).xlsx'\n",
    "SHEET = 'Form Responses 1'\n",
    "\n",
    "df = pd.read_excel(FILE, sheet_name=SHEET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e6a84",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ MAIN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e332d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_COL_RE = re.compile(\n",
    "    r'^(Q\\d+)\\.\\s*(.*?)\\s*\\[Method\\s*([12])\\s*-\\s*([A-Za-z]+)\\]$',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def normalize_header(col):\n",
    "    return re.sub(r'\\s+', ' ', str(col).replace('\\r', ' ')).strip()\n",
    "\n",
    "\n",
    "def parse_main_col(col):\n",
    "    s = normalize_header(col)\n",
    "    m = MAIN_COL_RE.match(s)\n",
    "    if not m:\n",
    "        return None\n",
    "    qid = m.group(1).upper()\n",
    "    qtext = m.group(2).strip()\n",
    "    method_num = int(m.group(3))\n",
    "    metric = m.group(4).title()\n",
    "    return qid, qtext, method_num, metric\n",
    "\n",
    "\n",
    "rating_cols, meta = [], []\n",
    "for c in df.columns:\n",
    "    parsed = parse_main_col(c)\n",
    "    if parsed:\n",
    "        rating_cols.append(c)\n",
    "        meta.append((c, *parsed))\n",
    "\n",
    "meta_df = pd.DataFrame(meta, columns=['col', 'QID', 'QuestionText', 'MethodNum', 'Metric'])\n",
    "meta_df['order'] = np.arange(len(meta_df))\n",
    "meta_df['IsQueryEval'] = False\n",
    "\n",
    "question_order = (\n",
    "    meta_df[['QuestionText', 'order']]\n",
    "    .drop_duplicates(subset=['QuestionText'])\n",
    "    .sort_values('order')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "if len(question_order) < 20:\n",
    "    raise ValueError(f\"Expected at least 20 unique questions, found {len(question_order)}.\")\n",
    "\n",
    "std1_questions = set(question_order.head(10)['QuestionText'])\n",
    "meta_df['Standard'] = np.where(\n",
    "    meta_df['QuestionText'].isin(std1_questions),\n",
    "    'ISO/IEC 27002',\n",
    "    'ISO/IEC 27017'\n",
    ")\n",
    "\n",
    "\n",
    "def method_name(row):\n",
    "    if row['Standard'] == 'ISO/IEC 27002':\n",
    "        return 'SSSL' if row['MethodNum'] == 1 else 'LLM'\n",
    "    return 'KNN (derived from ISO/IEC 27017)' if row['MethodNum'] == 1 else 'LLM'\n",
    "\n",
    "\n",
    "meta_df['Method'] = meta_df.apply(method_name, axis=1)\n",
    "\n",
    "long = df[rating_cols].melt(var_name='col', value_name='Score').merge(meta_df, on='col', how='left')\n",
    "long['Score'] = pd.to_numeric(long['Score'], errors='coerce')\n",
    "\n",
    "means = long.groupby(['IsQueryEval', 'Standard', 'Method', 'Metric', 'QID'], as_index=False)['Score'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e6690a",
   "metadata": {},
   "source": [
    "### Main table (Q1‚ÄìQ10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(x):\n",
    "        print(x)\n",
    "\n",
    "metric_order = [\n",
    "    m for m in ['Consistency', 'Completeness', 'Correctness', 'Generalizability', 'Precision']\n",
    "    if m in means['Metric'].unique()\n",
    "]\n",
    "q_order = sorted(means['QID'].dropna().unique(), key=lambda x: int(re.sub(r'\\D', '', str(x)) or 0))\n",
    "\n",
    "main = means[means['IsQueryEval'] == False].pivot_table(\n",
    "    index=['Standard', 'Method'],\n",
    "    columns=['Metric', 'QID'],\n",
    "    values='Score'\n",
    ")\n",
    "\n",
    "cols = [(m, q) for m in metric_order for q in q_order]\n",
    "main = main.reindex(columns=pd.MultiIndex.from_tuples(cols))\n",
    "\n",
    "# Average table: one value per metric (averaged across Q1..Q10), plus overall average.\n",
    "main_avg = (\n",
    "    means[means['IsQueryEval'] == False]\n",
    "    .groupby(['Standard', 'Method', 'Metric'], as_index=False)['Score']\n",
    "    .mean()\n",
    "    .pivot_table(index=['Standard', 'Method'], columns='Metric', values='Score')\n",
    ")\n",
    "main_avg = main_avg.reindex(columns=metric_order)\n",
    "main_avg['Overall_Avg'] = main_avg.mean(axis=1)\n",
    "\n",
    "print('Main table (by Metric x QID):')\n",
    "display(main)\n",
    "\n",
    "print('Average table (by Metric + Overall):')\n",
    "display(main_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7253d9",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ OTHER EVALUATION (Query winner comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec951c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_SCORE_RE = re.compile(\n",
    "    r'^Score\\s+the\\s+relevance.*?Query\\s*(\\d+)\\s*:.*?\\[Method\\s*([12])\\]$',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "QUERY_WINNER_RE = re.compile(\n",
    "    r'^Select\\s+which\\s+method\\s+performs\\s+better\\s+overall.*?Query\\s*(\\d+)\\s*:',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "query_score_cols = {}\n",
    "winner_cols = {}\n",
    "\n",
    "for c in df.columns:\n",
    "    s = normalize_header(c)\n",
    "    m = QUERY_SCORE_RE.match(s)\n",
    "    if m:\n",
    "        qnum = int(m.group(1))\n",
    "        method_num = int(m.group(2))\n",
    "        query_score_cols.setdefault(qnum, {})[method_num] = c\n",
    "        continue\n",
    "\n",
    "    w = QUERY_WINNER_RE.match(s)\n",
    "    if w:\n",
    "        qnum = int(w.group(1))\n",
    "        winner_cols[qnum] = c\n",
    "\n",
    "print('Detected query score columns:', {q: sorted(v.keys()) for q, v in query_score_cols.items()})\n",
    "print('Detected winner columns:', sorted(winner_cols.keys()))\n",
    "\n",
    "records = []\n",
    "for qnum in sorted(query_score_cols):\n",
    "    cols = query_score_cols[qnum]\n",
    "    if 1 not in cols or 2 not in cols:\n",
    "        continue\n",
    "\n",
    "    m1 = pd.to_numeric(df[cols[1]], errors='coerce')\n",
    "    m2 = pd.to_numeric(df[cols[2]], errors='coerce')\n",
    "\n",
    "    win_col = winner_cols.get(qnum)\n",
    "    if win_col is not None:\n",
    "        win = df[win_col].astype(str).str.strip().str.lower()\n",
    "        llm_wins = win.str.contains(r'\\bmethod\\s*1\\b|\\b1\\b', regex=True, na=False).sum()\n",
    "        similarity_wins = win.str.contains(r'\\bmethod\\s*2\\b|\\b2\\b', regex=True, na=False).sum()\n",
    "    else:\n",
    "        llm_wins = 0\n",
    "        similarity_wins = 0\n",
    "\n",
    "    records.append({\n",
    "        'Query': f'Query {qnum}',\n",
    "        'LLM_avg': m1.mean(),\n",
    "        'Similarity_avg': m2.mean(),\n",
    "        'LLM_wins': llm_wins,\n",
    "        'Similarity_wins': similarity_wins,\n",
    "    })\n",
    "\n",
    "other_eval = pd.DataFrame(\n",
    "    records,\n",
    "    columns=['Query', 'LLM_avg', 'Similarity_avg', 'LLM_wins', 'Similarity_wins']\n",
    ").sort_values('Query').reset_index(drop=True)\n",
    "\n",
    "if other_eval.empty:\n",
    "    print('No query-evaluation rows were parsed. Check QUERY_SCORE_RE / QUERY_WINNER_RE against headers.')\n",
    "    other_eval['Winner'] = pd.Series(dtype='object')\n",
    "else:\n",
    "    other_eval['Winner'] = np.select(\n",
    "        [\n",
    "            other_eval['LLM_wins'] > other_eval['Similarity_wins'],\n",
    "            other_eval['Similarity_wins'] > other_eval['LLM_wins'],\n",
    "        ],\n",
    "        ['LLM', 'Similarity'],\n",
    "        default='Tie'\n",
    "    )\n",
    "\n",
    "other_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef101eb",
   "metadata": {},
   "source": [
    "## üíæ Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18fc801",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('final_results.xlsx') as w:\n",
    "    main.to_excel(w, sheet_name='Main_Eval')\n",
    "    main_avg.to_excel(w, sheet_name='Main_Avg')\n",
    "    other_eval.to_excel(w, sheet_name='Other_Eval', index=False)\n",
    "\n",
    "print('Saved: final_results.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
