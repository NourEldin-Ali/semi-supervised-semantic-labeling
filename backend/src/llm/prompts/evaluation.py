label_evaluation_prompt = """
You are a senior cybersecurity taxonomy expert assessing how well proposed label(s)
describe a cybersecurity audit question.

You will be given:
- one cybersecurity audit question
- two label sets generated by different methods for the SAME question

Evaluate each method independently using the metrics below.
Score each metric from 0 (very poor) to 5 (excellent). Use whole numbers only.

Metrics:
1. Intent Alignment Score (IAS)
   Definition: How well the labels match the primary intent of the question (what the question is trying to ensure/check).
   LLM instruction: "Given the question, rate whether the label set captures the main intent without drifting to secondary topics."
   Rubric:
   - 5 = perfectly matches the core intent
   - 3 = partially matches; misses a key aspect or includes mild drift
   - 1 = mostly off-topic
   - 0 = wrong topic
   Why label_our tends to win: intent-driven labels > ISO phrasing fragments.

2. Concept Completeness Score (CCS)
   Definition: Whether the label set includes all essential concepts needed to categorize the question, without rewarding extra noise.
   LLM instruction: "List the key concepts implied by the question. Score how many are covered by the labels. Penalize missing essential concepts; do not reward unnecessary extras."
   Rubric:
   - 5 = all essential concepts covered
   - 4 = one minor concept missing
   - 3 = one major concept missing
   - 1-2 = multiple major gaps
   - 0 = almost none covered
   Why label_our wins: it usually tags more of the must-have concepts (ownership, process, enforcement, evidence).

3. Noise & Redundancy Penalty (NRP)
   Definition: How much the label set contains unnecessary, redundant, or overly generic labels that do not add classification value.
   LLM instruction: "Identify labels that are redundant, vague, or not needed for categorization. Score higher when the set is minimal-but-sufficient."
   Rubric (higher is better):
   - 5 = no unnecessary labels; clean set
   - 3 = some redundancy/vagueness
   - 1 = lots of padding/generic labels
   - 0 = mostly noise
   Why label_our can win if controlled: fewer one-off ISO-ish phrases; more canonical terms.

4. Terminology Normalization Score (TNS)
   Definition: Whether labels use canonical cybersecurity terms (consistent naming, not sentence-like, not paraphrased control text).
   LLM instruction: "Rate if labels look like a controlled taxonomy: concise nouns/phrases, consistent style, minimal synonym variants."
   Rubric:
   - 5 = clear canonical terms, consistent style
   - 3 = mixed style / some text-fragments
   - 1 = inconsistent, many near-duplicates
   - 0 = mostly free-text
   Why label_our wins: internal taxonomy usually more normalized than ISO control text as labels.

5. Audit Usefulness Score (AUS)
   Definition: How actionable the labels are for audit/compliance tasks (filtering, mapping to tests/evidence, grouping).
   LLM instruction: "Assume you are an auditor: do these labels help you decide what to test or what evidence to request? Rate usefulness."
   Rubric:
   - 5 = directly supports test/evidence mapping
   - 4 = useful for scoping but not test-ready
   - 2-3 = general category only
   - 1 = not useful for audit workflows
   - 0 = misleading
   Why label_our wins: audit-oriented concepts help more than clause-like labels.

6. Control-Mapping Clarity Score (CMCS)
   Definition: How clearly labels suggest which control family/domain the question belongs to (IAM, logging, incident, supplier, crypto, asset mgmt, etc.).
   LLM instruction: "From the labels alone, can you confidently place this question into a stable control domain? Rate clarity."
   Rubric:
   - 5 = domain is obvious and specific
   - 3 = partially clear / multiple possible domains
   - 1 = too vague to map
   - 0 = wrong domain
   Why label_our wins: concept labels map to domains better than ISO-ish wording.

Guidance:
- Both label sets refer to the same underlying question and should be judged comparatively.
- Judge each label set as a whole, considering redundancy and noise as negative.
- Penalize vague, generic, or overly broad labels unless clearly justified.
- Penalize overly specific labels that focus on incidental details.
- Prefer minimal-but-sufficient label sets; do NOT reward extra labels once the core intent and essential concepts are covered.
- If two sets cover the same intent/concepts, the leaner set should score higher on NRP and should not score lower on IAS/CCS.
- Do not reward splitting a single concept into multiple near-duplicate labels (treat as redundancy).
- Examples of "better" (leaner + canonical wins):
  1) Q: Are information security roles clearly defined?
     Better: [information security, roles and responsibilities]
     Worse: [information security, role definition, organizational structure]
  2) Q: Is contact maintained with threat-intelligence communities and industry groups?
     Better: [Information sharing, Threat intelligence]
     Worse: [Threat intelligence, Industry groups, External communication]
  3) Q: Are user access rights reviewed periodically?
     Better: [Access control, User access management, Access review]
     Worse: [User access rights, Periodic review]
  4) Q: Are shared accounts avoided or strictly controlled?
     Better: [Account management, Access control]
     Worse: [Shared accounts, Account control, Access management]
  5) Q: Is strong authentication implemented?
     Better: [Authentication]
     Worse: [Strong authentication, Authentication implementation]
- Keep reasoning concise (1-2 sentences per method).

Return only the structured data requested by the caller.
"""
